{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qwxv86zHXJHE"
      },
      "source": [
        "#Twitter and Reddit Sarcasm Detection Experimentation\n",
        "## Authors: Mark Biegel and Youssef Othman\n",
        "### CMSC 473 NLP Course Project\n",
        "\n",
        "<br> This notebook contains experimenation with NLP featurizers and machine learning models to determine which combination is the best for detecting sarcasm among social media posts from twitter and reddit data sets\n",
        "<br><br>*NOTE* Google Colab is requirement for running this noebook, as it uses the `google.colab.files` library which is unusable locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uI6u21eHXqYm"
      },
      "outputs": [],
      "source": [
        "### Import Cell\n",
        "\n",
        "# Colab import functionality\n",
        "from google.colab import files\n",
        "\n",
        "# Data processing tools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# SKLEARN packages\n",
        "import sklearn.feature_extraction.text as featExtract\n",
        "import sklearn.preprocessing as prep\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ezt8AASRE9UB"
      },
      "outputs": [],
      "source": [
        "# Suppress Scikit-Learn's Forced Warnings (unnecssary for current experimentation)\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GzUVKfL8PKn",
        "outputId": "641491d5-5ce4-49f5-e6af-3d02553ac37c"
      },
      "outputs": [],
      "source": [
        "### Create Files for final dataframes and for resulting finalized model outputs\n",
        "\n",
        "final_stats_file = open(\"finalStats.txt\", 'w')\n",
        "fileSectionBreakString = \"-----------------------------------------------------------\\n\"\n",
        "final_stats_file.write(fileSectionBreakString)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qj-oGfwX7dWt"
      },
      "source": [
        "### IMPORTANT - PLEASE READ THIS:\n",
        "For this next cell, the data will be read in, so the `kaggle_data_key.json` needs to be uploaded when the output says to \"Choose file\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        },
        "id": "lef7Sl-nX8ol",
        "outputId": "c8b172af-d947-46c3-9170-5b70c586fde5"
      },
      "outputs": [],
      "source": [
        "### Reading data in\n",
        "\n",
        "# Get the training and test set from the first source\n",
        "!wget \"https://raw.githubusercontent.com/AmirAbaskohi/SemEval2022-Task6-Sarcasm-Detection/main/Data/Main%20Dataset/Train_Dataset.csv\"\n",
        "!wget \"https://raw.githubusercontent.com/AmirAbaskohi/SemEval2022-Task6-Sarcasm-Detection/main/Data/Test_Dataset.csv\"\n",
        "\n",
        "# Get the training set from the second source\n",
        "!wget \"https://raw.githubusercontent.com/surajr/SarcasmDetection/master/Data/sarcasm-dataset.txt\"\n",
        "\n",
        "\n",
        "\n",
        "## NOTE: Third and fourth dataset are downloaded from Kaggle online\n",
        "  # ONLY NEED TO RUN ONCE TO GET FILES; DON'T THINK WE NEED IT FOR SUBMISSION\n",
        "  \n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "# Then move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download and extract data files\n",
        "!kaggle datasets download -d danofer/sarcasm\n",
        "!unzip sarcasm.zip\n",
        "!kaggle datasets download -d rmisra/news-headlines-dataset-for-sarcasm-detection\n",
        "!unzip news-headlines-dataset-for-sarcasm-detection.zip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD2bz5otxXzc"
      },
      "source": [
        "# Data Preprocessing\n",
        "#### Train, dev, and test sets created"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oq2zxHea212"
      },
      "outputs": [],
      "source": [
        "### Processing first dataset\n",
        "\n",
        "# Use pandas to load in the data\n",
        "train_set1_df = pd.read_csv('Train_Dataset.csv')\n",
        "\n",
        "# Load in the test set into a cleaner version\n",
        "test_set1_df = pd.read_csv(\"Test_Dataset.csv\")\n",
        "\n",
        "# Defining Constants\n",
        "sarcasm_tweet_column = \"tweet\"\n",
        "is_sarcasm_column = \"sarcastic\"\n",
        "\n",
        "# Combining test and traing datasets\n",
        "cleaned_set1_train_df = train_set1_df[[sarcasm_tweet_column, is_sarcasm_column]].dropna()\n",
        "cleaned_set1_test_df = test_set1_df[[sarcasm_tweet_column, is_sarcasm_column]].dropna()\n",
        "\n",
        "total_cleaned_df = cleaned_set1_train_df.append(cleaned_set1_test_df, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-m-gHWXHxcCF"
      },
      "outputs": [],
      "source": [
        "### Processing second dataset\n",
        "  # Data was formatted weird, need to remove new line \n",
        "  # characters and separate out the sarcasm label\n",
        "\n",
        "with open('sarcasm-dataset.txt') as file:\n",
        "  df = file.readlines()\n",
        "file.close()\n",
        "\n",
        "for i in range(len(df)):\n",
        "  df[i] = df[i].strip(\"\\n\")\n",
        "\n",
        "second_set = []\n",
        "\n",
        "for line in df:\n",
        "  second_set.append([line[:-2], int(line[-1])])\n",
        "\n",
        "cleaned_second_set_df = pd.DataFrame(second_set, columns = ['tweet', 'sarcastic']).dropna()\n",
        "total_cleaned_df = total_cleaned_df.append(cleaned_second_set_df, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e5F0D5SxcQ3"
      },
      "outputs": [],
      "source": [
        "### Processing third dataset\n",
        "\n",
        "third_set_df = pd.read_csv(\"train-balanced-sarcasm.csv\")\n",
        "\n",
        "# Combining test and traing datasets\n",
        "cleaned_set3_train_df = third_set_df[['comment', 'label']].rename(columns={'comment': 'tweet', 'label': 'sarcastic'}).dropna()\n",
        "total_cleaned_df = total_cleaned_df.append(cleaned_set3_train_df, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2szHiMcwfZb"
      },
      "outputs": [],
      "source": [
        "### Processing fourth dataset\n",
        "\n",
        "fourth_set_df = pd.read_json('Sarcasm_Headlines_Dataset_v2.json', lines=True)\n",
        "\n",
        "# Combining test and traing datasets\n",
        "cleaned_set4_train_df = fourth_set_df[['headline', 'is_sarcastic']].rename(columns={'headline': 'tweet', 'is_sarcastic': 'sarcastic'}).dropna()\n",
        "total_cleaned_df = total_cleaned_df.append(cleaned_set4_train_df, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "NqnxyJYGXEvq",
        "outputId": "f755c27c-368f-4227-946a-d28fa77278ac"
      },
      "outputs": [],
      "source": [
        "# Printing out final compiled dataframe\n",
        "total_cleaned_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96qRl2WyxzPL",
        "outputId": "071b1998-ac5a-4d26-b4d7-6216237b9123"
      },
      "outputs": [],
      "source": [
        "# Create train, dev, and test sets\n",
        "TRAIN_SET_RATIO = 0.8\n",
        "DEV_SET_RATIO = 0.10\n",
        "TEST_SET_RATION = 0.10\n",
        "\n",
        "train_set_split_index = int(len(total_cleaned_df)*TRAIN_SET_RATIO)\n",
        "dev_set_split_index = int(train_set_split_index+ (len(total_cleaned_df) * DEV_SET_RATIO))\n",
        "\n",
        "train = total_cleaned_df[0:train_set_split_index]\n",
        "dev = total_cleaned_df[train_set_split_index: dev_set_split_index]\n",
        "test = total_cleaned_df[dev_set_split_index:]\n",
        "\n",
        "print(\"The following values should be equal:\", (len(train)+len(dev)+len(test)), \"=\", len(total_cleaned_df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVxqVVs0xqmO"
      },
      "source": [
        "# Vectorization/Featurization of Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yD5861zTX_Wu",
        "outputId": "dbd796ee-6eff-4120-a165-39138734a6d4"
      },
      "outputs": [],
      "source": [
        "### Vectorization/Featurization methods\n",
        "    # Need rationale for why these vectorization/featurization methods are picked\n",
        "    # Tune hyperparameters of each method????????\n",
        "\n",
        "## Sentiment Analysis\n",
        "  # Count Vectorizer\n",
        "print(\"[Count Vectorizer] Encoding data...\", end=\"\")\n",
        "CountVectorizer = featExtract.CountVectorizer()\n",
        "CV_train_vectors = CountVectorizer.fit_transform(train[sarcasm_tweet_column])\n",
        "CV_dev_vectors = CountVectorizer.transform(dev[sarcasm_tweet_column])\n",
        "CV_test_vectors = CountVectorizer.transform(test[sarcasm_tweet_column])\n",
        "print(\"done\")\n",
        "\n",
        "# One Hot Encoding\n",
        "print(\"[One Hot Encoder] Encoding data...\", end=\"\")\n",
        "OneHotEncoder = prep.OneHotEncoder(handle_unknown='ignore')\n",
        "OHE_train_vectors = OneHotEncoder.fit_transform(train[[sarcasm_tweet_column]])\n",
        "OHE_dev_vectors = OneHotEncoder.transform(dev[[sarcasm_tweet_column]])\n",
        "OHE_test_vectors = OneHotEncoder.transform(test[[sarcasm_tweet_column]])\n",
        "print(\"done\")\n",
        "\n",
        "# Word Vectorizaztion/Extraction\n",
        "   #TfidfVectorizer\n",
        "print(\"[Tfidf Vectorizer] Encoding data...\", end=\"\")\n",
        "tfidf_vectorizer = featExtract.TfidfVectorizer()\n",
        "Tfidf_train_vectors = tfidf_vectorizer.fit_transform(train[sarcasm_tweet_column])\n",
        "Tfidf_dev_vectors = tfidf_vectorizer.transform(dev[sarcasm_tweet_column])\n",
        "Tfidf_test_vectors = tfidf_vectorizer.transform(test[sarcasm_tweet_column])\n",
        "print(\"done\")\n",
        "\n",
        "   #HashingVectorizer\n",
        "print(\"[Hashing Vectorizer] Encoding data...\", end=\"\")\n",
        "hashing_vectorizer = featExtract.HashingVectorizer(n_features=2**4)\n",
        "hashing_train_vectors = hashing_vectorizer.fit_transform(train[sarcasm_tweet_column])\n",
        "hashing_dev_vectors = hashing_vectorizer.transform(dev[sarcasm_tweet_column])\n",
        "hashing_test_vectors = hashing_vectorizer.transform(test[sarcasm_tweet_column])\n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yapkP9dhFBO1"
      },
      "source": [
        "---\n",
        "# Individual Featurizer Analysis"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "73VTfPfLFB2W"
      },
      "source": [
        "### Model Predictor Function\n",
        "\n",
        "A reusable function to predict on each model, gather statistics, and make matplotlib graphs.\n",
        "<br>This function is SUPER helpful for systematically tuning the hyper-parameters of each model, as different models, prediction vectors, and statistical outpus can be gathered and specified by the parameters passed in. This made the experimenation process for observing and gathering data from the four featurizers and three classifiers simplified and consistent.\n",
        "\n",
        "<br>Statistics for each model written to text file named `finalStats.txt`\n",
        "<br>Images of the plots from each model saved to local directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufQ1N49dCeP2"
      },
      "outputs": [],
      "source": [
        "def model_Predicting(classifier, predict_vectors, correct_labels, model_name=\"ML Model\", model_labels=[], matplot_filename=\"output.png\", show_stats=True, make_plot=False, save_plot=False):\n",
        "  # Multinomial NB Hyperparameter development\n",
        "  pred = classifier.predict(predict_vectors)\n",
        "\n",
        "  acc_score = metrics.accuracy_score(correct_labels.to_numpy(), pred)\n",
        "  precision_score = metrics.precision_score(correct_labels.to_numpy(), pred, average='macro')\n",
        "  recall_score = metrics.recall_score(correct_labels.to_numpy(), pred, average='macro')\n",
        "  f1_score = metrics.f1_score(correct_labels.to_numpy(), pred, average='macro')\n",
        "  roc_score = metrics.roc_auc_score(correct_labels.to_numpy(), pred)\n",
        "  roc_auc = metrics.roc_auc_score(correct_labels.to_numpy(), pred)\n",
        "\n",
        "\n",
        "  # If the boolean flag for showing outputs\n",
        "  # is true, display matplotlib plot and metric data\n",
        "  if (show_stats):\n",
        "    print(\"Prediction:\", pred)    \n",
        "    print('Total accuracy classification score: {}'.format(acc_score))\n",
        "    print('Total precision score: {}'.format(precision_score))\n",
        "    print('Total recall score: {}'.format(recall_score))\n",
        "    print('Total F1 classification score: {}'.format(f1_score))\n",
        "    print('Total ROC classification score: {}'.format(roc_score))\n",
        "    print('Total AUC of ROC classification: {}'.format(roc_auc))\n",
        "\n",
        "  if(make_plot):\n",
        "    # Get data for ROC Curve\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(correct_labels.to_numpy(), pred)\n",
        "\n",
        "    # Create ROC curve\n",
        "    plt.plot(fpr, tpr)\n",
        "\n",
        "    # Add axis labels to plot\n",
        "    title = model_name + \" ROC Curve\"\n",
        "    plt.title(title)\n",
        "    plt.legend(model_labels)\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "\n",
        "    # Display plot and save plot as jpg\n",
        "  if (save_plot):\n",
        "    plt.savefig(matplot_filename)\n",
        "  \n",
        "  return {\"accuracy\": acc_score, \"precision\": precision_score, \"recall\": recall_score, \"f1\": f1_score, \"roc\": roc_score, \"roc_auc\": roc_auc}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7rRXFXn-06M"
      },
      "outputs": [],
      "source": [
        "# Train, dev, and test instances of sarcasm classifcation from dataset\n",
        "train_is_sarcasm = train[is_sarcasm_column]\n",
        "dev_is_sarcasm = dev[is_sarcasm_column]\n",
        "test_is_sarcasm = test[is_sarcasm_column]\n",
        "\n",
        "# A list of all train, dev, and test vectors from each featurizer\n",
        "# Used for systematic hyper-parameter tuning\n",
        "all_vectorizer_train_vectors = [CV_train_vectors, OHE_train_vectors, Tfidf_train_vectors, hashing_train_vectors]\n",
        "all_vectorizer_dev_vectors = [CV_dev_vectors, OHE_dev_vectors, Tfidf_dev_vectors, hashing_dev_vectors]\n",
        "all_vectorizer_test_vectors = [CV_test_vectors, OHE_test_vectors, Tfidf_test_vectors, hashing_test_vectors]\n",
        "all_vectorizer_names = [\"CountVectorizer\", \"OneHotEncoder\", \"TfidfVectorizer\", \"HashingVectorizer\"]\n",
        "\n",
        "# Final featurizer results for each model\n",
        "featurization_multiNB_final_results = {}\n",
        "featurization_SGD_final_results = {}\n",
        "featurization_LogReg_final_results = {}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2NTCmXK1BlM1"
      },
      "source": [
        "## Training, developing, and testing all Vectorization Methods\n",
        "\n",
        "A list of hyperparameters were chosen for tuning, each with\n",
        "a set of values to be tested on. A loop goes through all of these \n",
        "combinations and store the result of each parameter combination in \n",
        "order to find the best-in-class combination to use for testing each model.\n",
        "\n",
        "In order to determine best-in-class hyperparameter combination, each\n",
        "iteration creates an ROC AUC score that is stored in order to directly\n",
        "compare the results across all iterations.\n",
        "\n",
        "Each model is trained on training set, with hyperparameter tuning tested on dev set, testing the final evaluation of each iteration on the test set"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multinomial Naive Bayes Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UIbp8kQsYCHM",
        "outputId": "b2bb9932-6d8d-4c89-b466-62f1290726e8"
      },
      "outputs": [],
      "source": [
        "### MULTINOMIAL NAIVE BAYES\n",
        "\n",
        "\n",
        "# Hyper parameters for tuning\n",
        "alpha_values = np.linspace(0.00000001, 100, 250)\n",
        "fit_prior_list = [True, False]\n",
        "\n",
        "# Have to use one leass model for MultiNB since HashVectorizer did not work\n",
        "NUM_MODELS_USED = len(all_vectorizer_train_vectors)-1\n",
        "\n",
        "# Loops over each vectorizer to gather roc_auc_score data for comparison EXCEPT FOR LAST ONE (HashVectorizer)\n",
        "for i in range(NUM_MODELS_USED):\n",
        "\n",
        "  print(\"\\n\\n----------------------------------------- VECTOR MODEL:\", all_vectorizer_names[i], \"-----------------------------------------\\n\")\n",
        "  print(\"Iteration Count:\", end=\" \")\n",
        "\n",
        "  # AUC ROC metric list that holds one vecotrizer's roc_auc_score and\n",
        "  # then resets when moving to the next vectorizer since the it is\n",
        "  # used to find the best combination of hyperparameters for each model\n",
        "  auc_roc_nb_resulting_metric = []\n",
        "  count = 0\n",
        "\n",
        "  # Iterate over all combinations of values between the 2 parameters' lists \n",
        "  for curr_alpha in alpha_values:\n",
        "    for curr_bool_fit in fit_prior_list:\n",
        "      \n",
        "      # Show every 50th iteration (nicely formatted)\n",
        "      if (count % 50 == 0):\n",
        "        print(count, end=\" \")\n",
        "        \n",
        "      # Naive Bayes training\n",
        "      multiNB_clf = MultinomialNB(alpha=curr_alpha, fit_prior=curr_bool_fit)\n",
        "      multiNB_clf.fit(all_vectorizer_train_vectors[i], train_is_sarcasm)\n",
        "\n",
        "      # Multi NB Prediction with all dev vector sets\n",
        "      metrics_dict = model_Predicting(classifier=multiNB_clf, predict_vectors=all_vectorizer_dev_vectors[i], correct_labels=dev_is_sarcasm, show_stats=False)\n",
        "\n",
        "      # Adding ROC_AUC metric to globalized list\n",
        "      auc_roc_nb_resulting_metric.append(metrics_dict[\"roc_auc\"])\n",
        "      count += 1\n",
        "    \n",
        "\n",
        "  # Finding max combination\n",
        "  max_roc_index = np.where(auc_roc_nb_resulting_metric == max(auc_roc_nb_resulting_metric))[0][0]\n",
        "\n",
        "  # Find alpha and fit_prior best combination indices\n",
        "  alpha_index_loc = int(np.floor(max_roc_index/len(fit_prior_list)))\n",
        "  fit_prior_index_loc = max_roc_index % len(fit_prior_list)\n",
        "\n",
        "  # Extract alpha and fit_prior best combination values\n",
        "  alpha_combo = alpha_values[alpha_index_loc]\n",
        "  fit_prior_combo = fit_prior_list[fit_prior_index_loc]\n",
        "  print(\"\\nMax index:\", max_roc_index, \"with a value\", max(auc_roc_nb_resulting_metric))\n",
        "  print(\"Alpha combination:\", alpha_combo)\n",
        "  print(\"Fit Prior combination:\", fit_prior_combo)\n",
        "\n",
        "  # Using best hyperparameter combination, run model on test set\n",
        "  best_hyper_param_combo = MultinomialNB(alpha=alpha_combo, fit_prior=fit_prior_combo)\n",
        "  best_hyper_param_combo.fit(all_vectorizer_train_vectors[i], train_is_sarcasm)\n",
        "\n",
        "  # Testing best combination hyperparameters on NB model with test data\n",
        "  print(\"\\nTESTING MULTINB MODEL\")\n",
        "  model_name = \"MultiNB\"\n",
        "\n",
        "  # If on the last iteration, save plot\n",
        "  if (i == (NUM_MODELS_USED-1)):\n",
        "    plot_filename = model_name + \"_roc_curve.jpg\"\n",
        "    metrics_dict = model_Predicting(classifier=best_hyper_param_combo, predict_vectors=all_vectorizer_test_vectors[i], correct_labels=test_is_sarcasm, model_name=model_name, model_labels=all_vectorizer_names[:NUM_MODELS_USED], make_plot=True, save_plot=True, matplot_filename=plot_filename)\n",
        "  \n",
        "  # Otherwise, just add to plot\n",
        "  else:\n",
        "    metrics_dict = model_Predicting(classifier=best_hyper_param_combo, predict_vectors=all_vectorizer_test_vectors[i], correct_labels=test_is_sarcasm, model_name=model_name, model_labels=all_vectorizer_names[:NUM_MODELS_USED], make_plot=True)\n",
        "\n",
        "  # Add the each combination's ROC AUC score to a global dictionary\n",
        "  featurization_multiNB_final_results[all_vectorizer_names[i]] = metrics_dict[\"roc_auc\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU9IpfJv4-yx",
        "outputId": "21940d75-e5a8-4bc3-a100-cd376b400a12"
      },
      "outputs": [],
      "source": [
        "### Finds the best-in-class featurizer for MultiNB model to \n",
        "# be written to a text file containing final statistics\n",
        "\n",
        "ROUND_TO = 5\n",
        "max_key = max(featurization_multiNB_final_results, key=featurization_multiNB_final_results.get)\n",
        "max_value = round(max(featurization_multiNB_final_results.values()), ROUND_TO)\n",
        "\n",
        "print(featurization_multiNB_final_results)\n",
        "print(\"\\n\",max_key, \"was the best model with an ROC AUC of:\", max_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OX6FsOwhaUda"
      },
      "outputs": [],
      "source": [
        "# Set up string to write stats to file\n",
        "fileTotalStatString = \"Total,\" + model_name + \",\" + str(featurization_multiNB_final_results) +\"\\n\"\n",
        "fileBestModelString = \"Best,\" + model_name + \",\" + str(max_key) + \",\" + str(max_value) +\"\\n\"\n",
        "\n",
        "# Write to file\n",
        "final_stats_file.write(model_name + \"\\n\")\n",
        "final_stats_file.write(fileTotalStatString)\n",
        "final_stats_file.write(fileBestModelString)\n",
        "final_stats_file.write(fileSectionBreakString)\n",
        "final_stats_file.close()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eLrYFqTljHNC"
      },
      "source": [
        "### Stochastic Gradient Descent Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gqeVfUb9hDVL",
        "outputId": "782e7d04-9b4f-41c5-b5a2-747621b93daf"
      },
      "outputs": [],
      "source": [
        "# SGD Model Training\n",
        "### WARNING: With the number of combinations of\n",
        "  # hyper-parameters, this code could take awhile to run\n",
        "loss_values = ['hinge', 'log', 'modified_huber', 'perceptron', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive']\n",
        "alpha_values = np.linspace(0.0001, 0.8, 10)\n",
        "learning_rate_values = ['constant', 'optimal', 'invscaling']\n",
        "MAX_ITER = 6000\n",
        "ETA0 = 1\n",
        "NUM_MODELS_USED = len(all_vectorizer_train_vectors)\n",
        "\n",
        "# Loops over each vectorizer to gather roc_auc_score data for comparison EXCEPT FOR LAST ONE (HashVectorizer)\n",
        "for i in range(NUM_MODELS_USED):\n",
        "\n",
        "  print(\"\\n\\n----------------------------------------- VECTOR MODEL:\", all_vectorizer_names[i], \"-----------------------------------------\\n\")\n",
        "  print(\"Iteration Count:\", end=\" \")\n",
        "\n",
        "  # AUC ROC metric list that holds one vecotrizer's roc_auc_score and\n",
        "  # then resets when moving to the next vectorizer since the it is\n",
        "  # used to find the best combination of hyperparameters for each model\n",
        "  auc_roc_SGD_resulting_metric = {}\n",
        "  count = 0\n",
        "\n",
        "  # Iterate over all combinations of values between the 2 parameters' lists \n",
        "  for curr_loss in loss_values:    \n",
        "      for curr_alpha in alpha_values:\n",
        "        for curr_learning_rate in learning_rate_values:\n",
        "\n",
        "          # Show every 3rd iteration (nicely formatted)\n",
        "          if (count % 28 == 0 and count != 0):\n",
        "            print(count)\n",
        "          elif (count % 3 == 0):\n",
        "            #print(count, curr_loss, curr_learning_rate, curr_alpha, end=\"\\n\")\n",
        "            print(count,end=\" \")\n",
        "          \n",
        "          # Logistic Regression training\n",
        "          SGD_clf = SGDClassifier(random_state=0, loss=curr_loss, max_iter=MAX_ITER, alpha=curr_alpha, learning_rate=curr_learning_rate, eta0=ETA0)\n",
        "          SGD_clf.fit(all_vectorizer_train_vectors[i], train_is_sarcasm)\n",
        "\n",
        "          # Logistic Regression Prediction with all dev vector sets\n",
        "          metrics_dict = model_Predicting(classifier=SGD_clf, predict_vectors=all_vectorizer_dev_vectors[i], correct_labels=dev_is_sarcasm, show_stats=False)\n",
        "\n",
        "          # Adding ROC_AUC metric to globalized dictionary\n",
        "          hyperParamIterCombo = str(curr_loss) + \",\" + str(curr_alpha) + \",\" + str(curr_learning_rate)\n",
        "          auc_roc_SGD_resulting_metric[hyperParamIterCombo] = metrics_dict[\"roc_auc\"]\n",
        "          count += 1\n",
        "\n",
        "\n",
        "  # Finding max combination\n",
        "  max_key = max(auc_roc_SGD_resulting_metric, key=auc_roc_SGD_resulting_metric.get)\n",
        "  max_value = max(auc_roc_SGD_resulting_metric.values())\n",
        "\n",
        "  best_values = max_key.split(\",\")\n",
        "\n",
        "  best_loss = best_values[0]\n",
        "  best_alpha = float(best_values[1])\n",
        "  best_learning_rate = best_values[2]\n",
        "\n",
        "  # Extract C combination value\n",
        "  print(\"\\nMax Settings:\", max_key, \"being a value of\", max_value)\n",
        "  print(\"Best values:\", best_values)\n",
        "\n",
        "  # Using best hyperparameter combination, run model on test set\n",
        "  best_hyper_param_combo = SGDClassifier(random_state=0, loss=best_loss, max_iter=MAX_ITER, alpha=best_alpha, learning_rate=best_learning_rate, eta0=ETA0)\n",
        "  best_hyper_param_combo.fit(all_vectorizer_train_vectors[i], train_is_sarcasm)\n",
        "\n",
        "  # Testing best combination hyperparameters on NB model with test data\n",
        "  print(\"\\nTESTING SGD MODEL\")\n",
        "  model_name = \"SGD\"\n",
        "\n",
        "  # If on the last iteration, save plot\n",
        "  if (i == (NUM_MODELS_USED-1)):\n",
        "    plot_filename = model_name + \"_roc_curve.jpg\"\n",
        "    metrics_dict = model_Predicting(classifier=best_hyper_param_combo, predict_vectors=all_vectorizer_test_vectors[i], correct_labels=test_is_sarcasm, model_name=model_name, model_labels=all_vectorizer_names[:NUM_MODELS_USED], make_plot=True, save_plot=True, matplot_filename=plot_filename)\n",
        "  \n",
        "  # Otherwise, just add to plot\n",
        "  else:\n",
        "    metrics_dict = model_Predicting(classifier=best_hyper_param_combo, predict_vectors=all_vectorizer_test_vectors[i], correct_labels=test_is_sarcasm, model_name=model_name, model_labels=all_vectorizer_names[:NUM_MODELS_USED], make_plot=True)\n",
        "  \n",
        "  featurization_SGD_final_results[all_vectorizer_names[i]] = metrics_dict[\"roc_auc\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xseQeX-_jLUS",
        "outputId": "40a311d6-aab9-40cf-a1c0-55071489bf53"
      },
      "outputs": [],
      "source": [
        "### Finds the best-in-class featurizer for SGD model to \n",
        "# be written to a text file containing final statistics\n",
        "\n",
        "ROUND_TO = 5\n",
        "max_key = max(featurization_SGD_final_results, key=featurization_SGD_final_results.get)\n",
        "max_value = round(max(featurization_SGD_final_results.values()), ROUND_TO)\n",
        "\n",
        "print(featurization_SGD_final_results)\n",
        "print(\"\\n\",max_key, \"was the best model with an ROC AUC of:\", max_value)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPlka2tgaSQa"
      },
      "outputs": [],
      "source": [
        "# Set up string to write stats to file\n",
        "fileTotalStatString = \"Total,\" + model_name + \",\" + str(featurization_SGD_final_results) +\"\\n\"\n",
        "fileBestModelString = \"Best,\" + model_name + \",\" + str(max_key) + \",\" + str(max_value) +\"\\n\"\n",
        "\n",
        "# Write to file\n",
        "final_stats_file = open(\"finalStats.txt\", 'a')\n",
        "final_stats_file.write(model_name + \"\\n\")\n",
        "final_stats_file.write(fileTotalStatString)\n",
        "final_stats_file.write(fileBestModelString)\n",
        "final_stats_file.write(fileSectionBreakString)\n",
        "final_stats_file.close()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "z33493R3jLAb"
      },
      "source": [
        "### Logistic Regression Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "U-kpHuwHgMWM",
        "outputId": "a318c362-e404-4c6b-de46-2621287a0a9c"
      },
      "outputs": [],
      "source": [
        "### Logistic Regression\n",
        "  # DUE TO INSUFFICENT COMPUTING POWER, NOT AS MANY HYPERPARAMS WERE TESTED AS DESIRED\n",
        "C_values = np.linspace(0.00000001, 10, 3)\n",
        "MAX_ITERATIONS = 500\n",
        "NUM_MODELS_USED = len(all_vectorizer_train_vectors)\n",
        "\n",
        "# Loops over each vectorizer to gather roc_auc_score data for comparison EXCEPT FOR LAST ONE (HashVectorizer)\n",
        "for i in range(NUM_MODELS_USED):\n",
        "\n",
        "  print(\"\\n\\n----------------------------------------- VECTOR MODEL:\", all_vectorizer_names[i], \"-----------------------------------------\\n\")\n",
        "  print(\"Iteration Count:\", end=\" \")\n",
        "\n",
        "  # AUC ROC metric list that holds one vecotrizer's roc_auc_score and\n",
        "  # then resets when moving to the next vectorizer since the it is\n",
        "  # used to find the best combination of hyperparameters for each model\n",
        "  auc_roc_logreg_resulting_metric = []\n",
        "  count = 0\n",
        "\n",
        "  # Iterate over all combinations of values between the 2 parameters' lists \n",
        "  for curr_C in C_values:    \n",
        "    # Show every iteration\n",
        "    print(count, end=\" \")\n",
        "      \n",
        "    # Logistic Regression training\n",
        "    LogReg_clf = LogisticRegression(random_state=0, solver='lbfgs', C=curr_C, max_iter=MAX_ITERATIONS)\n",
        "    LogReg_clf.fit(all_vectorizer_train_vectors[i], train_is_sarcasm)\n",
        "\n",
        "    # Logistic Regression Prediction with all dev vector sets\n",
        "    metrics_dict = model_Predicting(classifier=LogReg_clf, predict_vectors=all_vectorizer_dev_vectors[i], correct_labels=dev_is_sarcasm, show_stats=False)\n",
        "\n",
        "    # Adding ROC_AUC metric to globalized list\n",
        "    auc_roc_logreg_resulting_metric.append(metrics_dict[\"roc_auc\"])\n",
        "    count += 1\n",
        "\n",
        "\n",
        "  # Finding max combination\n",
        "  print(\"\\nROC AUC scores:\", auc_roc_logreg_resulting_metric)\n",
        "  max_roc_index = np.where(auc_roc_logreg_resulting_metric == max(auc_roc_logreg_resulting_metric))[0][0]\n",
        "\n",
        "  # Find index location of max value for C (inverse regularization)\n",
        "  C_index_loc = int(np.floor(max_roc_index/len(fit_prior_list)))+1\n",
        "\n",
        "  # Extract C combination value\n",
        "  C_combo = C_values[C_index_loc]\n",
        "  print(\"\\nMax index:\", max_roc_index, \"being a value of\", max(auc_roc_logreg_resulting_metric))\n",
        "  print(\"C combination:\", C_combo)\n",
        "\n",
        "  # Using best hyperparameter combination, run model on test set\n",
        "  best_hyper_param_combo = LogisticRegression(random_state=0, solver='lbfgs', C=C_combo, max_iter=MAX_ITERATIONS)\n",
        "  best_hyper_param_combo.fit(all_vectorizer_train_vectors[i], train_is_sarcasm)\n",
        "\n",
        "  # Testing best combination hyperparameters on NB model with test data\n",
        "  print(\"\\nTESTING LOGISTIC REGRESSION MODEL\")\n",
        "  model_name = \"LogReg\"\n",
        "\n",
        "  # If on the last iteration, save plot\n",
        "  if (i == (NUM_MODELS_USED-1)):\n",
        "\n",
        "    plot_filename = model_name + \"_roc_curve.jpg\"\n",
        "    metrics_dict = model_Predicting(classifier=best_hyper_param_combo, predict_vectors=all_vectorizer_test_vectors[i], correct_labels=test_is_sarcasm, model_name=model_name, model_labels=all_vectorizer_names[:NUM_MODELS_USED], make_plot=True, save_plot=True, matplot_filename=plot_filename)\n",
        "  \n",
        "  # Otherwise, just add to plot\n",
        "  else:\n",
        "    metrics_dict = model_Predicting(classifier=best_hyper_param_combo, predict_vectors=all_vectorizer_test_vectors[i], correct_labels=test_is_sarcasm, model_name=model_name, model_labels=all_vectorizer_names[:NUM_MODELS_USED], make_plot=True)\n",
        "\n",
        "  featurization_LogReg_final_results[all_vectorizer_names[i]] = metrics_dict[\"roc_auc\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Isu0lrkdKod"
      },
      "outputs": [],
      "source": [
        "### Finds the best-in-class featurizer for Logistic Regression \n",
        "# model to be written to a text file containing final statistics\n",
        "\n",
        "ROUND_TO = 5\n",
        "max_key = max(featurization_LogReg_final_results, key=featurization_LogReg_final_results.get)\n",
        "max_value = round(max(featurization_LogReg_final_results.values()), ROUND_TO)\n",
        "\n",
        "print(featurization_LogReg_final_results)\n",
        "print(\"\\n\",max_key, \"was the best model with an ROC AUC of:\", max_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9VflWSXaNMT"
      },
      "outputs": [],
      "source": [
        "# Set up string to write stats to file\n",
        "fileTotalStatString = \"Total,\" + model_name + \",\" + str(featurization_LogReg_final_results) +\"\\n\"\n",
        "fileBestModelString = \"Best,\" + model_name + \",\" + str(max_key) + \",\" + str(max_value) +\"\\n\"\n",
        "\n",
        "# Write to file\n",
        "final_stats_file = open(\"finalStats.txt\", 'a')\n",
        "final_stats_file.write(model_name + \"\\n\")\n",
        "final_stats_file.write(fileTotalStatString)\n",
        "final_stats_file.write(fileBestModelString)\n",
        "final_stats_file.write(fileSectionBreakString)\n",
        "final_stats_file.close()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Statistics for each model written to text file named `finalStats.txt`\n",
        "<br> Images of the plots from each model saved to local directory"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "KvweBocUKI7u"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
